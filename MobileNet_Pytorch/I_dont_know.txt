isinstance 뭐임?
> isinstance(m, nn.Conv2d)    m이 conv2d이면 True를 반환한다. 아니라면 False를 반환한다.

 nn.init.kaiming_normal_ ?
 nn.init.constant_  ?
 nn.init.normal_   ?
 >>> 가중치 초기 설정해주는 함수들.  ( https://supermemi.tistory.com/121 ) 가중치 설명해주는 링크



torch.ne
> 각 텐서의 요소(element)들을 element-wise로 각각 비교해 다르면 True를, 같으면 False를 반환한다. 
- 형태 : torch.ne(비교 대상 tensor, 비교할 tensor나 value, *, out=None) → Tensor
- torch.not_equal과 동일
EX) 
>>> torch.ne(torch.tensor([[2, 5], [4, 3]]), torch.tensor([[2, 8], [2, 3]]))
# 결과 : tensor([[False, True], [True, False]])


torch.eq
> ne와 반대로 각 텐서의 요소(element)들을 비교해 같으면 True를, 다르면 False를 반환한다.
- 형태 : torch.eq(비교 대상 tensor, 비교할 tensor나 value, *, out=None) → Tensor



Pytorch에서 Dataloader를 사용하는 이유?
dataloader class는 batch기반의 딥러닝 학습을 위해 mini batch를 만들어주는 역할이다.
dataloader를 통해 dataset의 전체 데이터가 batch size로 slice된다. 
앞서 만들었던 dataset을 input으로 넣어주면 여러 옵션(데이터 묶기, 섞기, 병렬처리)을 통해 batch를 만들어준다.



[pytorch]
nn.Module에는 train time과 evaluate time에 수행하는 다른 작업을 switching해줄 수 있도록하는 함수를 제공한다.
train time과 evaluate time에 서로 다르게 동작해야 하는 것들에는 대표적으로 아래와 같은 것들이 있다.
- Dropout layer
- BatchNorm layer



=====================================================================================================================================

def train(model, train_loader, optimizer):

    model.train()

    for batch_idx, (data, target) in enumerate(train_loader):

        data, target = data.to(DEVICE), target.to(DEVICE)

        optimizer.zero_grad()

        output = model(data)

        loss = F.cross_entropy(output, target)

        loss.backward()

        optimizer.step()



model.train() 
≫ 모델을 학습 모드로 변환 / 평가 모드는 model.eval() 로 할 수 있다.

 
data, target = data.to(DEVICE), target.to(DEVICE)
≫  각 data 와 target 을 앞서 설정한 DEVICE(GPU 혹은 CPU) 에 보내는 것

 
optimizer.zero_grad()
≫ 반복 때마다 기울기를 새로 계산하므로, 이 함수로 초기화

 
output = model(data)
≫ data를 모델에 넣어서 가설(hypothesis)를 획득

 
loss = F.cross_entropy(output, target)
≫ 가설과 groud truth를 비교하여 loss 계산


loss.backward()
≫ loss 를 역전파 알고리즘으로 계산


optimizer.step()
≫  계산한 기울기를 앞서 정의한 알고리즘에 맞추어 가중치를 수정

=================================================================================================================================================================







